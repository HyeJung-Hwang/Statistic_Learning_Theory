{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gradient_descent.ipynb","provenance":[],"authorship_tag":"ABX9TyO8c5sabDOscdA1Yt4QBepj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"I0GualI7YE4L"},"source":["# h(x)=theta*x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPsycVdRZK22","executionInfo":{"status":"ok","timestamp":1631104703754,"user_tz":-540,"elapsed":7,"user":{"displayName":"황혜정","photoUrl":"","userId":"00027110760378319944"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eg04KhYBdZrk"},"source":["1. Batch Gradient Descent"]},{"cell_type":"code","metadata":{"id":"Yy9059rmYQzj"},"source":["def gradientDescent(X,y,theta,alpha,num_iters):\n","  #sample data: X,y (둘다 (m,1)벡터)\n","  #theta: h(x) 계수\n","  #alpha: learning rate\n","  #num_iters: 반복하고 싶은 횟수\n","  m=y.size\n","  for i in range(num_iters):\n","    y_hat=np.dot(X,theta)  # predict\n","    theta=theta-alpha*np.dot(X.T,y_hat-y) #(learningrate)*(h(x)를 theta에 대해 속미분)*오차\n","  return rate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e39NLshjdgkK"},"source":["2. Stochastic Gradient Descent"]},{"cell_type":"code","metadata":{"id":"dC8wrI4Tbm6O","executionInfo":{"status":"ok","timestamp":1631105284080,"user_tz":-540,"elapsed":450,"user":{"displayName":"황혜정","photoUrl":"","userId":"00027110760378319944"}}},"source":["def SGD(f,theta0,alpha,num_iters):\n","  # f- 최적화할 함수, i: theta , o: cost & gradient\n","  # theta0: initial point\n","  start_iter=0\n","  theta=theta0\n","  for iter in range(start_iter+1,num_iters+1):\n","    _,grad=f(theta)\n","    theta=theta-(alpha*grad)\n","  return theta"],"execution_count":5,"outputs":[]}]}