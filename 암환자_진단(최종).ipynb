{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "암환자_진단(최종).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJung-Hwang/Statistic_Learning_Theory/blob/main/%EC%95%94%ED%99%98%EC%9E%90_%EC%A7%84%EB%8B%A8(%EC%B5%9C%EC%A2%85).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E70BU9pzUpqN",
        "outputId": "c610da6d-b69f-4a43-aad1-cd930879c53d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dil34lm2V1T6"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_0dFbHdrBg"
      },
      "source": [
        "# Training dataset 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "CHqtsBP8du2s",
        "outputId": "68a6801b-d42c-4c6f-8813-03d92224ddab"
      },
      "source": [
        "# upload file with dictionary format, with key as name of uploaded file \n",
        "# and corresponding values as the contens of the file \n",
        "from google.colab import files\n",
        "uploaded_train = files.upload()\n",
        "uploaded_test = files.upload()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4cf374f5-5ac6-4e66-9abb-969043e4fb64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4cf374f5-5ac6-4e66-9abb-969043e4fb64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Train_Data.txt to Train_Data.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-919c8faf-7e1f-4489-ba2b-15d07469661d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-919c8faf-7e1f-4489-ba2b-15d07469661d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Test_Data.txt to Test_Data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlNQjWYg-y6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559a9176-178e-4799-8171-7482be10d087"
      },
      "source": [
        "# use panda and io pachage to load txt. \n",
        "df_train = pd.read_csv(io.StringIO(uploaded_train['Train_Data.txt'].decode(\"utf-8\")), \n",
        "                       sep=',', names=['ID', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'class'], header=None)\n",
        "print(df_train)\n",
        "\n",
        "df_test = pd.read_csv(io.StringIO(uploaded_test['Test_Data.txt'].decode(\"utf-8\")), \n",
        "                       sep=',', names=['ID', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'class'], header=None)\n",
        "print(df_test)\n",
        "#replace missing value ? with 0\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ID  x1  x2  x3  x4  x5  x6  x7  x8  x9  class\n",
            "0    1000025   5   1   1   1   2   1   3   1   1      2\n",
            "1    1002945   5   4   4   5   7  10   3   2   1      2\n",
            "2    1015425   3   1   1   1   2   2   3   1   1      2\n",
            "3    1016277   6   8   8   1   3   4   3   7   1      2\n",
            "4    1017023   4   1   1   3   2   1   3   1   1      2\n",
            "..       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..    ...\n",
            "495  1170945   3   1   1   1   1   1   2   1   1      2\n",
            "496  1181567   1   1   1   1   1   1   1   1   1      2\n",
            "497  1182404   4   2   1   1   2   1   1   1   1      2\n",
            "498  1204558   4   1   1   1   2   1   2   1   1      2\n",
            "499  1217952   4   1   1   1   2   1   2   1   1      2\n",
            "\n",
            "[500 rows x 11 columns]\n",
            "          ID  x1  x2  x3  x4  x5 x6  x7  x8  x9  class\n",
            "0    1224565   6   1   1   1   2  1   3   1   1      2\n",
            "1    1238186   4   1   1   1   2  1   2   1   1      2\n",
            "2    1253917   4   1   1   2   2  1   2   1   1      2\n",
            "3    1265899   4   1   1   1   2  1   3   1   1      2\n",
            "4    1268766   1   1   1   1   2  1   1   1   1      2\n",
            "..       ...  ..  ..  ..  ..  .. ..  ..  ..  ..    ...\n",
            "194   776715   3   1   1   1   3  2   1   1   1      2\n",
            "195   841769   2   1   1   1   2  1   1   1   1      2\n",
            "196   888820   5  10  10   3   7  3   8  10   2      4\n",
            "197   897471   4   8   6   4   3  4  10   6   1      4\n",
            "198   897471   4   8   8   5   4  5  10   4   1      4\n",
            "\n",
            "[199 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXYIBh9jw9Hq"
      },
      "source": [
        "## Feature Selection or Manipulation\n",
        "사이킷런 feature_selection 라이브러리의\n",
        "SelectFromModel 함수 이용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JqzGB1hMiTC"
      },
      "source": [
        "(1) train, test data 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsHFLJ9oulM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b41791e-699a-4c5e-cef2-39d89288e469"
      },
      "source": [
        "X = df_train[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']].replace({'?':0})\n",
        "# print(X['x6'][0:10])\n",
        "\n",
        "y = df_train['class'].replace({2:0, 4:1})\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "## feature 선택\n",
        "x_train = torch.Tensor(np.array([X['x1'],X['x2'], X['x3'], X['x4'], X['x5'],X['x6'], X['x7'], X['x8'], X['x9']]).astype(np.uint8)).t()  # str to unit, [3, 500] ->  [500, 3] ,X['x2'],X['x3'],X['x7']\n",
        "print(x_train.shape)\n",
        "\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "y_train = torch.Tensor(y).unsqueeze(1) # [500] -> [500, 1]\n",
        "print(y_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([500, 9])\n",
            "torch.Size([500, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_c2-z5Mpb-"
      },
      "source": [
        "(2) C=0.01, penalty=default로 feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDMxe2DTM1IJ"
      },
      "source": [
        "#필요 라이브러리 import\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3c3GmbiPcAm",
        "outputId": "aeacb610-ce55-4b17-fa06-43bdc9c59497"
      },
      "source": [
        "selector = SelectFromModel(estimator=LogisticRegression(C=0.01,solver='liblinear')).fit(x_train, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf5_PJyxQjOf",
        "outputId": "9e6bebde-9587-4d06-c0f0-81c04169faab"
      },
      "source": [
        "selector.get_support()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True, False, False,  True,  True,  True, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature=np.array(['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9'])\n",
        "selected_feature=[]\n",
        "for i in range(9):\n",
        "    if selector.get_support()[i]:\n",
        "        selected_feature.append(feature[i])"
      ],
      "metadata": {
        "id": "X2tWN-db3Ejg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5omtMvSH3XF5",
        "outputId": "203e8569-7215-4570-e35f-4c93da83ee3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x2', 'x5', 'x6', 'x7']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gkj4yqKlgcA"
      },
      "source": [
        "# 최종적으로 결정된 feature들로 train accuracy와 test accuracy 계산\n",
        "lambda를 0.01,0.1,0,01로 달리하며 L1 regularization과 L2 regularization을 각각 수행하여 test accuracy 비교\n",
        "\n",
        "\n",
        "**정확한 error 값 계산 위해 추가적으로 validation accuracy도 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chyeXOu3OPrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c63296-cab9-4416-97f0-110eb7e329f8"
      },
      "source": [
        "X = df_train[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']].replace({'?':0})\n",
        "# print(X['x6'][0:10])\n",
        "\n",
        "y = df_train['class'].replace({2:0, 4:1})\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "## feature 선택\n",
        "x_train = torch.Tensor(np.array([X['x2'],X['x5'],X['x6'],X['x7']]).astype(np.uint8)).t()  # str to unit, [3, 500] ->  [500, 3] ,X['x2'],X['x3'],X['x7']\n",
        "print(x_train.shape)\n",
        "y_train = torch.Tensor(y).unsqueeze(1) # [500] -> [500, 1]\n",
        "print(y_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([500, 4])\n",
            "torch.Size([500, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkO3R5eaOd6d"
      },
      "source": [
        "Train Data 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLZL7UMMVB8U",
        "outputId": "17913d3d-7957-4c02-92b0-472f8b1ae31f"
      },
      "source": [
        "X = df_train[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']].replace({'?':0})\n",
        "# print(X['x6'][0:10])\n",
        "\n",
        "y = df_train['class'].replace({2:0, 4:1})\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "## feature 선택\n",
        "x_train = torch.Tensor(np.array([X['x2'],X['x5'],X['x6'],X['x7']]).astype(np.uint8)).t()  # str to unit, [3, 500] ->  [500, 3] ,X['x2'],X['x3'],X['x7']\n",
        "\n",
        "x_train,x_valid=x_train[0:399],x_train[399:]\n",
        "print(x_train.shape)\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "y_train = torch.Tensor(y).unsqueeze(1) # [500] -> [500, 1]\n",
        "y_train,y_valid=y_train[0:399],y_train[399:]\n",
        "print(y_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([399, 4])\n",
            "torch.Size([399, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model class\n",
        "z = w1*x1 + w2*x2 + w3*x3 ....  + w6*x6 + b  -> <br>\n",
        "y = a = sigma(z) -> <br>\n",
        "L(y_hat = a, y)"
      ],
      "metadata": {
        "id": "h_RfhgAy43cr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHhAaCTAevR_",
        "outputId": "ac0262b1-862f-4cf7-e2a4-6ab4c5502cc4"
      },
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self, x): \n",
        "    pred = torch.sigmoid(self.linear(x))\n",
        "    return pred   #probability (not direct value)\n",
        "\n",
        "  def predict(self, x):\n",
        "    pred = self.forward(x)\n",
        "    if pred >= 0.5:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# instantiate model class\n",
        "torch.manual_seed(1)\n",
        "model = LogisticRegression(x_train.shape[1], 1) # [500, 2]\n",
        "print(list(model.parameters()))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([399, 4])\n",
            "[Parameter containing:\n",
            "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4707], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# function to get model parameters (w1, w2,w3,w4, b)"
      ],
      "metadata": {
        "id": "ViZ6MfRV4_98"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rycu36_qew8x",
        "outputId": "955051ec-6317-4eab-8a44-3843101242bb"
      },
      "source": [
        "[w, b] = model.parameters() # Ws, bias\n",
        "print(w) \n",
        "w1, w2,w3,w4= w.view(x_train.shape[1])\n",
        "\n",
        "def get_params():\n",
        "  return (w1.item(), w2.item(),w3.item(),w4.item(), b[0].item())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "2-ommvOM5GAU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkW2eQbklixb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe42f8f-a142-4a71-c0b7-5f1044e3f16b"
      },
      "source": [
        "\n",
        "# instantitate optimizer \n",
        "criterion = nn.BCELoss()  # = nn.CrossEntropyLoss() # for LR with more than 2 classes\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.02,momentum=0.9) #weight_decay=1e-5 -> L2 regularizer\n",
        "\n",
        "# training the model \n",
        "epochs = 500\n",
        "losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  y_pred = model.forward(x_train)\n",
        "  # calculate loss \n",
        "  loss = criterion(y_pred, y_train)\n",
        "  \n",
        "  la=0.01\n",
        "  l_reg = torch.tensor(0.)\n",
        "  for param in model.parameters():\n",
        "      l_reg += torch.norm(param)  # for L1 regularizer : torch.norm(param, 1) \n",
        "  loss += la * l_reg\n",
        "\n",
        "\n",
        "  print(\"epoch: \", i, \"loss: \", loss.item())\n",
        "  losses.append(loss.item())\n",
        "\n",
        "  optimizer.zero_grad() # clear gradients wrt parameters\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 loss:  0.6204988956451416\n",
            "epoch:  1 loss:  0.5941654443740845\n",
            "epoch:  2 loss:  0.5661634206771851\n",
            "epoch:  3 loss:  0.5505428314208984\n",
            "epoch:  4 loss:  0.5420894026756287\n",
            "epoch:  5 loss:  0.5306088328361511\n",
            "epoch:  6 loss:  0.5110642313957214\n",
            "epoch:  7 loss:  0.48489102721214294\n",
            "epoch:  8 loss:  0.457746297121048\n",
            "epoch:  9 loss:  0.43603992462158203\n",
            "epoch:  10 loss:  0.42289993166923523\n",
            "epoch:  11 loss:  0.4157584011554718\n",
            "epoch:  12 loss:  0.40877971053123474\n",
            "epoch:  13 loss:  0.3985211253166199\n",
            "epoch:  14 loss:  0.3863321840763092\n",
            "epoch:  15 loss:  0.37556353211402893\n",
            "epoch:  16 loss:  0.368126779794693\n",
            "epoch:  17 loss:  0.363742858171463\n",
            "epoch:  18 loss:  0.36102426052093506\n",
            "epoch:  19 loss:  0.3586127460002899\n",
            "epoch:  20 loss:  0.3557252883911133\n",
            "epoch:  21 loss:  0.3522156774997711\n",
            "epoch:  22 loss:  0.34838563203811646\n",
            "epoch:  23 loss:  0.34471115469932556\n",
            "epoch:  24 loss:  0.34158873558044434\n",
            "epoch:  25 loss:  0.33917176723480225\n",
            "epoch:  26 loss:  0.3373357057571411\n",
            "epoch:  27 loss:  0.3357704281806946\n",
            "epoch:  28 loss:  0.33414649963378906\n",
            "epoch:  29 loss:  0.3322678208351135\n",
            "epoch:  30 loss:  0.3301352858543396\n",
            "epoch:  31 loss:  0.3279034495353699\n",
            "epoch:  32 loss:  0.32577377557754517\n",
            "epoch:  33 loss:  0.3238924741744995\n",
            "epoch:  34 loss:  0.32230013608932495\n",
            "epoch:  35 loss:  0.32094067335128784\n",
            "epoch:  36 loss:  0.3197084367275238\n",
            "epoch:  37 loss:  0.3185010254383087\n",
            "epoch:  38 loss:  0.3172577917575836\n",
            "epoch:  39 loss:  0.31597253680229187\n",
            "epoch:  40 loss:  0.3146827518939972\n",
            "epoch:  41 loss:  0.31344521045684814\n",
            "epoch:  42 loss:  0.3123077154159546\n",
            "epoch:  43 loss:  0.3112899363040924\n",
            "epoch:  44 loss:  0.3103771209716797\n",
            "epoch:  45 loss:  0.30952996015548706\n",
            "epoch:  46 loss:  0.3087027668952942\n",
            "epoch:  47 loss:  0.30786195397377014\n",
            "epoch:  48 loss:  0.3069964647293091\n",
            "epoch:  49 loss:  0.30611661076545715\n",
            "epoch:  50 loss:  0.3052443265914917\n",
            "epoch:  51 loss:  0.30440011620521545\n",
            "epoch:  52 loss:  0.3035944104194641\n",
            "epoch:  53 loss:  0.3028244376182556\n",
            "epoch:  54 loss:  0.3020778298377991\n",
            "epoch:  55 loss:  0.30133911967277527\n",
            "epoch:  56 loss:  0.3005962371826172\n",
            "epoch:  57 loss:  0.2998442053794861\n",
            "epoch:  58 loss:  0.2990853190422058\n",
            "epoch:  59 loss:  0.2983272969722748\n",
            "epoch:  60 loss:  0.29757845401763916\n",
            "epoch:  61 loss:  0.2968451976776123\n",
            "epoch:  62 loss:  0.29612934589385986\n",
            "epoch:  63 loss:  0.29542869329452515\n",
            "epoch:  64 loss:  0.29473844170570374\n",
            "epoch:  65 loss:  0.2940540015697479\n",
            "epoch:  66 loss:  0.29337239265441895\n",
            "epoch:  67 loss:  0.29269352555274963\n",
            "epoch:  68 loss:  0.29201939702033997\n",
            "epoch:  69 loss:  0.29135316610336304\n",
            "epoch:  70 loss:  0.2906974256038666\n",
            "epoch:  71 loss:  0.2900533974170685\n",
            "epoch:  72 loss:  0.28942087292671204\n",
            "epoch:  73 loss:  0.28879818320274353\n",
            "epoch:  74 loss:  0.28818339109420776\n",
            "epoch:  75 loss:  0.287574827671051\n",
            "epoch:  76 loss:  0.28697171807289124\n",
            "epoch:  77 loss:  0.28637415170669556\n",
            "epoch:  78 loss:  0.2857828736305237\n",
            "epoch:  79 loss:  0.2851986289024353\n",
            "epoch:  80 loss:  0.28462204337120056\n",
            "epoch:  81 loss:  0.2840532064437866\n",
            "epoch:  82 loss:  0.28349176049232483\n",
            "epoch:  83 loss:  0.28293687105178833\n",
            "epoch:  84 loss:  0.28238776326179504\n",
            "epoch:  85 loss:  0.2818439304828644\n",
            "epoch:  86 loss:  0.2813051640987396\n",
            "epoch:  87 loss:  0.28077152371406555\n",
            "epoch:  88 loss:  0.28024324774742126\n",
            "epoch:  89 loss:  0.2797205150127411\n",
            "epoch:  90 loss:  0.2792035937309265\n",
            "epoch:  91 loss:  0.27869242429733276\n",
            "epoch:  92 loss:  0.27818673849105835\n",
            "epoch:  93 loss:  0.27768629789352417\n",
            "epoch:  94 loss:  0.2771908640861511\n",
            "epoch:  95 loss:  0.2767002284526825\n",
            "epoch:  96 loss:  0.2762143015861511\n",
            "epoch:  97 loss:  0.27573317289352417\n",
            "epoch:  98 loss:  0.2752569317817688\n",
            "epoch:  99 loss:  0.2747855484485626\n",
            "epoch:  100 loss:  0.274319052696228\n",
            "epoch:  101 loss:  0.2738575041294098\n",
            "epoch:  102 loss:  0.27340060472488403\n",
            "epoch:  103 loss:  0.27294841408729553\n",
            "epoch:  104 loss:  0.2725007236003876\n",
            "epoch:  105 loss:  0.2720574736595154\n",
            "epoch:  106 loss:  0.271618515253067\n",
            "epoch:  107 loss:  0.27118393778800964\n",
            "epoch:  108 loss:  0.2707537114620209\n",
            "epoch:  109 loss:  0.2703278660774231\n",
            "epoch:  110 loss:  0.269906222820282\n",
            "epoch:  111 loss:  0.2694888710975647\n",
            "epoch:  112 loss:  0.2690756320953369\n",
            "epoch:  113 loss:  0.2686665654182434\n",
            "epoch:  114 loss:  0.2682614028453827\n",
            "epoch:  115 loss:  0.2678602933883667\n",
            "epoch:  116 loss:  0.2674630880355835\n",
            "epoch:  117 loss:  0.2670697271823883\n",
            "epoch:  118 loss:  0.2666802704334259\n",
            "epoch:  119 loss:  0.26629453897476196\n",
            "epoch:  120 loss:  0.26591262221336365\n",
            "epoch:  121 loss:  0.2655344009399414\n",
            "epoch:  122 loss:  0.26515993475914\n",
            "epoch:  123 loss:  0.26478901505470276\n",
            "epoch:  124 loss:  0.2644217312335968\n",
            "epoch:  125 loss:  0.2640579342842102\n",
            "epoch:  126 loss:  0.26369765400886536\n",
            "epoch:  127 loss:  0.2633408010005951\n",
            "epoch:  128 loss:  0.2629874348640442\n",
            "epoch:  129 loss:  0.2626373767852783\n",
            "epoch:  130 loss:  0.2622906565666199\n",
            "epoch:  131 loss:  0.261947363615036\n",
            "epoch:  132 loss:  0.26160728931427\n",
            "epoch:  133 loss:  0.2612704038619995\n",
            "epoch:  134 loss:  0.26093676686286926\n",
            "epoch:  135 loss:  0.26060622930526733\n",
            "epoch:  136 loss:  0.2602788805961609\n",
            "epoch:  137 loss:  0.2599546015262604\n",
            "epoch:  138 loss:  0.2596333622932434\n",
            "epoch:  139 loss:  0.2593151926994324\n",
            "epoch:  140 loss:  0.25899994373321533\n",
            "epoch:  141 loss:  0.25868767499923706\n",
            "epoch:  142 loss:  0.2583783268928528\n",
            "epoch:  143 loss:  0.2580718994140625\n",
            "epoch:  144 loss:  0.25776830315589905\n",
            "epoch:  145 loss:  0.25746750831604004\n",
            "epoch:  146 loss:  0.2571695148944855\n",
            "epoch:  147 loss:  0.25687432289123535\n",
            "epoch:  148 loss:  0.2565818428993225\n",
            "epoch:  149 loss:  0.25629204511642456\n",
            "epoch:  150 loss:  0.2560049295425415\n",
            "epoch:  151 loss:  0.25572049617767334\n",
            "epoch:  152 loss:  0.25543859601020813\n",
            "epoch:  153 loss:  0.25515928864479065\n",
            "epoch:  154 loss:  0.2548825740814209\n",
            "epoch:  155 loss:  0.2546083629131317\n",
            "epoch:  156 loss:  0.2543366849422455\n",
            "epoch:  157 loss:  0.25406742095947266\n",
            "epoch:  158 loss:  0.253800630569458\n",
            "epoch:  159 loss:  0.2535362243652344\n",
            "epoch:  160 loss:  0.2532742917537689\n",
            "epoch:  161 loss:  0.25301462411880493\n",
            "epoch:  162 loss:  0.25275731086730957\n",
            "epoch:  163 loss:  0.2525023818016052\n",
            "epoch:  164 loss:  0.25224968791007996\n",
            "epoch:  165 loss:  0.25199925899505615\n",
            "epoch:  166 loss:  0.2517510652542114\n",
            "epoch:  167 loss:  0.2515050768852234\n",
            "epoch:  168 loss:  0.25126129388809204\n",
            "epoch:  169 loss:  0.2510197162628174\n",
            "epoch:  170 loss:  0.25078022480010986\n",
            "epoch:  171 loss:  0.2505428194999695\n",
            "epoch:  172 loss:  0.2503075897693634\n",
            "epoch:  173 loss:  0.2500744163990021\n",
            "epoch:  174 loss:  0.2498432695865631\n",
            "epoch:  175 loss:  0.2496141493320465\n",
            "epoch:  176 loss:  0.24938705563545227\n",
            "epoch:  177 loss:  0.24916192889213562\n",
            "epoch:  178 loss:  0.24893879890441895\n",
            "epoch:  179 loss:  0.24871762096881866\n",
            "epoch:  180 loss:  0.24849830567836761\n",
            "epoch:  181 loss:  0.24828091263771057\n",
            "epoch:  182 loss:  0.24806547164916992\n",
            "epoch:  183 loss:  0.24785183370113373\n",
            "epoch:  184 loss:  0.24764001369476318\n",
            "epoch:  185 loss:  0.24743008613586426\n",
            "epoch:  186 loss:  0.247221902012825\n",
            "epoch:  187 loss:  0.24701550602912903\n",
            "epoch:  188 loss:  0.24681097269058228\n",
            "epoch:  189 loss:  0.24660813808441162\n",
            "epoch:  190 loss:  0.24640701711177826\n",
            "epoch:  191 loss:  0.2462075650691986\n",
            "epoch:  192 loss:  0.24600987136363983\n",
            "epoch:  193 loss:  0.24581386148929596\n",
            "epoch:  194 loss:  0.24561947584152222\n",
            "epoch:  195 loss:  0.245426744222641\n",
            "epoch:  196 loss:  0.2452356368303299\n",
            "epoch:  197 loss:  0.24504615366458893\n",
            "epoch:  198 loss:  0.24485823512077332\n",
            "epoch:  199 loss:  0.24467191100120544\n",
            "epoch:  200 loss:  0.24448716640472412\n",
            "epoch:  201 loss:  0.24430391192436218\n",
            "epoch:  202 loss:  0.2441222369670868\n",
            "epoch:  203 loss:  0.2439420521259308\n",
            "epoch:  204 loss:  0.24376340210437775\n",
            "epoch:  205 loss:  0.24358618259429932\n",
            "epoch:  206 loss:  0.24341045320034027\n",
            "epoch:  207 loss:  0.2432362139225006\n",
            "epoch:  208 loss:  0.2430633306503296\n",
            "epoch:  209 loss:  0.24289190769195557\n",
            "epoch:  210 loss:  0.24272194504737854\n",
            "epoch:  211 loss:  0.24255332350730896\n",
            "epoch:  212 loss:  0.2423860877752304\n",
            "epoch:  213 loss:  0.24222025275230408\n",
            "epoch:  214 loss:  0.24205569922924042\n",
            "epoch:  215 loss:  0.24189259111881256\n",
            "epoch:  216 loss:  0.241730734705925\n",
            "epoch:  217 loss:  0.24157018959522247\n",
            "epoch:  218 loss:  0.24141094088554382\n",
            "epoch:  219 loss:  0.24125303328037262\n",
            "epoch:  220 loss:  0.2410963624715805\n",
            "epoch:  221 loss:  0.24094095826148987\n",
            "epoch:  222 loss:  0.24078680574893951\n",
            "epoch:  223 loss:  0.24063393473625183\n",
            "epoch:  224 loss:  0.24048221111297607\n",
            "epoch:  225 loss:  0.240331768989563\n",
            "epoch:  226 loss:  0.24018248915672302\n",
            "epoch:  227 loss:  0.24003440141677856\n",
            "epoch:  228 loss:  0.23988747596740723\n",
            "epoch:  229 loss:  0.2397417277097702\n",
            "epoch:  230 loss:  0.23959718644618988\n",
            "epoch:  231 loss:  0.2394537627696991\n",
            "epoch:  232 loss:  0.23931145668029785\n",
            "epoch:  233 loss:  0.23917026817798615\n",
            "epoch:  234 loss:  0.23903021216392517\n",
            "epoch:  235 loss:  0.23889127373695374\n",
            "epoch:  236 loss:  0.23875340819358826\n",
            "epoch:  237 loss:  0.23861655592918396\n",
            "epoch:  238 loss:  0.23848086595535278\n",
            "epoch:  239 loss:  0.23834621906280518\n",
            "epoch:  240 loss:  0.23821260035037994\n",
            "epoch:  241 loss:  0.23808002471923828\n",
            "epoch:  242 loss:  0.2379484921693802\n",
            "epoch:  243 loss:  0.2378179430961609\n",
            "epoch:  244 loss:  0.23768842220306396\n",
            "epoch:  245 loss:  0.2375599592924118\n",
            "epoch:  246 loss:  0.23743240535259247\n",
            "epoch:  247 loss:  0.23730584979057312\n",
            "epoch:  248 loss:  0.23718032240867615\n",
            "epoch:  249 loss:  0.237055703997612\n",
            "epoch:  250 loss:  0.23693203926086426\n",
            "epoch:  251 loss:  0.2368093580007553\n",
            "epoch:  252 loss:  0.236687570810318\n",
            "epoch:  253 loss:  0.23656679689884186\n",
            "epoch:  254 loss:  0.23644685745239258\n",
            "epoch:  255 loss:  0.23632784187793732\n",
            "epoch:  256 loss:  0.23620975017547607\n",
            "epoch:  257 loss:  0.23609255254268646\n",
            "epoch:  258 loss:  0.2359762191772461\n",
            "epoch:  259 loss:  0.23586082458496094\n",
            "epoch:  260 loss:  0.23574620485305786\n",
            "epoch:  261 loss:  0.2356324940919876\n",
            "epoch:  262 loss:  0.2355196624994278\n",
            "epoch:  263 loss:  0.23540765047073364\n",
            "epoch:  264 loss:  0.23529648780822754\n",
            "epoch:  265 loss:  0.2351861596107483\n",
            "epoch:  266 loss:  0.2350766360759735\n",
            "epoch:  267 loss:  0.2349679172039032\n",
            "epoch:  268 loss:  0.23486006259918213\n",
            "epoch:  269 loss:  0.23475296795368195\n",
            "epoch:  270 loss:  0.23464667797088623\n",
            "epoch:  271 loss:  0.23454119265079498\n",
            "epoch:  272 loss:  0.23443642258644104\n",
            "epoch:  273 loss:  0.23433248698711395\n",
            "epoch:  274 loss:  0.23422929644584656\n",
            "epoch:  275 loss:  0.23412686586380005\n",
            "epoch:  276 loss:  0.23402521014213562\n",
            "epoch:  277 loss:  0.2339242845773697\n",
            "epoch:  278 loss:  0.23382410407066345\n",
            "epoch:  279 loss:  0.23372463881969452\n",
            "epoch:  280 loss:  0.23362591862678528\n",
            "epoch:  281 loss:  0.23352789878845215\n",
            "epoch:  282 loss:  0.23343059420585632\n",
            "epoch:  283 loss:  0.233334019780159\n",
            "epoch:  284 loss:  0.23323814570903778\n",
            "epoch:  285 loss:  0.2331429272890091\n",
            "epoch:  286 loss:  0.2330484241247177\n",
            "epoch:  287 loss:  0.23295460641384125\n",
            "epoch:  288 loss:  0.2328614592552185\n",
            "epoch:  289 loss:  0.2327689826488495\n",
            "epoch:  290 loss:  0.2326771765947342\n",
            "epoch:  291 loss:  0.23258599638938904\n",
            "epoch:  292 loss:  0.23249554634094238\n",
            "epoch:  293 loss:  0.2324056625366211\n",
            "epoch:  294 loss:  0.2323164939880371\n",
            "epoch:  295 loss:  0.2322278916835785\n",
            "epoch:  296 loss:  0.2321399450302124\n",
            "epoch:  297 loss:  0.23205263912677765\n",
            "epoch:  298 loss:  0.23196591436862946\n",
            "epoch:  299 loss:  0.2318798452615738\n",
            "epoch:  300 loss:  0.23179443180561066\n",
            "epoch:  301 loss:  0.2317095547914505\n",
            "epoch:  302 loss:  0.2316252589225769\n",
            "epoch:  303 loss:  0.23154160380363464\n",
            "epoch:  304 loss:  0.23145852982997894\n",
            "epoch:  305 loss:  0.23137599229812622\n",
            "epoch:  306 loss:  0.23129409551620483\n",
            "epoch:  307 loss:  0.23121275007724762\n",
            "epoch:  308 loss:  0.23113198578357697\n",
            "epoch:  309 loss:  0.2310517579317093\n",
            "epoch:  310 loss:  0.23097212612628937\n",
            "epoch:  311 loss:  0.23089301586151123\n",
            "epoch:  312 loss:  0.23081448674201965\n",
            "epoch:  313 loss:  0.23073646426200867\n",
            "epoch:  314 loss:  0.23065903782844543\n",
            "epoch:  315 loss:  0.2305821031332016\n",
            "epoch:  316 loss:  0.23050570487976074\n",
            "epoch:  317 loss:  0.23042987287044525\n",
            "epoch:  318 loss:  0.23035453259944916\n",
            "epoch:  319 loss:  0.23027971386909485\n",
            "epoch:  320 loss:  0.23020538687705994\n",
            "epoch:  321 loss:  0.230131596326828\n",
            "epoch:  322 loss:  0.23005832731723785\n",
            "epoch:  323 loss:  0.2299855649471283\n",
            "epoch:  324 loss:  0.22991327941417694\n",
            "epoch:  325 loss:  0.2298414558172226\n",
            "epoch:  326 loss:  0.22977016866207123\n",
            "epoch:  327 loss:  0.22969934344291687\n",
            "epoch:  328 loss:  0.2296290099620819\n",
            "epoch:  329 loss:  0.22955916821956635\n",
            "epoch:  330 loss:  0.2294897735118866\n",
            "epoch:  331 loss:  0.22942087054252625\n",
            "epoch:  332 loss:  0.2293524295091629\n",
            "epoch:  333 loss:  0.22928446531295776\n",
            "epoch:  334 loss:  0.22921693325042725\n",
            "epoch:  335 loss:  0.22914984822273254\n",
            "epoch:  336 loss:  0.22908326983451843\n",
            "epoch:  337 loss:  0.22901709377765656\n",
            "epoch:  338 loss:  0.2289513796567917\n",
            "epoch:  339 loss:  0.22888611257076263\n",
            "epoch:  340 loss:  0.22882123291492462\n",
            "epoch:  341 loss:  0.2287568300962448\n",
            "epoch:  342 loss:  0.22869285941123962\n",
            "epoch:  343 loss:  0.22862932085990906\n",
            "epoch:  344 loss:  0.22856615483760834\n",
            "epoch:  345 loss:  0.22850346565246582\n",
            "epoch:  346 loss:  0.22844116389751434\n",
            "epoch:  347 loss:  0.2283792644739151\n",
            "epoch:  348 loss:  0.22831781208515167\n",
            "epoch:  349 loss:  0.22825676202774048\n",
            "epoch:  350 loss:  0.22819606959819794\n",
            "epoch:  351 loss:  0.22813579440116882\n",
            "epoch:  352 loss:  0.22807596623897552\n",
            "epoch:  353 loss:  0.22801651060581207\n",
            "epoch:  354 loss:  0.22795742750167847\n",
            "epoch:  355 loss:  0.2278987169265747\n",
            "epoch:  356 loss:  0.22784040868282318\n",
            "epoch:  357 loss:  0.22778251767158508\n",
            "epoch:  358 loss:  0.22772496938705444\n",
            "epoch:  359 loss:  0.22766782343387604\n",
            "epoch:  360 loss:  0.22761103510856628\n",
            "epoch:  361 loss:  0.2275545597076416\n",
            "epoch:  362 loss:  0.22749853134155273\n",
            "epoch:  363 loss:  0.22744283080101013\n",
            "epoch:  364 loss:  0.22738751769065857\n",
            "epoch:  365 loss:  0.22733250260353088\n",
            "epoch:  366 loss:  0.227277934551239\n",
            "epoch:  367 loss:  0.22722366452217102\n",
            "epoch:  368 loss:  0.22716975212097168\n",
            "epoch:  369 loss:  0.2271161675453186\n",
            "epoch:  370 loss:  0.22706295549869537\n",
            "epoch:  371 loss:  0.2270100712776184\n",
            "epoch:  372 loss:  0.2269575446844101\n",
            "epoch:  373 loss:  0.22690534591674805\n",
            "epoch:  374 loss:  0.22685348987579346\n",
            "epoch:  375 loss:  0.22680196166038513\n",
            "epoch:  376 loss:  0.22675076127052307\n",
            "epoch:  377 loss:  0.22669988870620728\n",
            "epoch:  378 loss:  0.22664934396743774\n",
            "epoch:  379 loss:  0.22659912705421448\n",
            "epoch:  380 loss:  0.2265492081642151\n",
            "epoch:  381 loss:  0.22649960219860077\n",
            "epoch:  382 loss:  0.22645032405853271\n",
            "epoch:  383 loss:  0.22640135884284973\n",
            "epoch:  384 loss:  0.226352721452713\n",
            "epoch:  385 loss:  0.22630438208580017\n",
            "epoch:  386 loss:  0.2262563407421112\n",
            "epoch:  387 loss:  0.22620859742164612\n",
            "epoch:  388 loss:  0.2261611521244049\n",
            "epoch:  389 loss:  0.22611403465270996\n",
            "epoch:  390 loss:  0.2260672003030777\n",
            "epoch:  391 loss:  0.2260206788778305\n",
            "epoch:  392 loss:  0.22597438097000122\n",
            "epoch:  393 loss:  0.22592845559120178\n",
            "epoch:  394 loss:  0.22588279843330383\n",
            "epoch:  395 loss:  0.22583737969398499\n",
            "epoch:  396 loss:  0.2257922887802124\n",
            "epoch:  397 loss:  0.2257474660873413\n",
            "epoch:  398 loss:  0.2257028967142105\n",
            "epoch:  399 loss:  0.22565867006778717\n",
            "epoch:  400 loss:  0.22561466693878174\n",
            "epoch:  401 loss:  0.225570946931839\n",
            "epoch:  402 loss:  0.22552751004695892\n",
            "epoch:  403 loss:  0.22548434138298035\n",
            "epoch:  404 loss:  0.22544144093990326\n",
            "epoch:  405 loss:  0.22539879381656647\n",
            "epoch:  406 loss:  0.22535642981529236\n",
            "epoch:  407 loss:  0.22531428933143616\n",
            "epoch:  408 loss:  0.22527240216732025\n",
            "epoch:  409 loss:  0.22523081302642822\n",
            "epoch:  410 loss:  0.2251894772052765\n",
            "epoch:  411 loss:  0.22514840960502625\n",
            "epoch:  412 loss:  0.2251075655221939\n",
            "epoch:  413 loss:  0.22506697475910187\n",
            "epoch:  414 loss:  0.22502663731575012\n",
            "epoch:  415 loss:  0.22498658299446106\n",
            "epoch:  416 loss:  0.22494667768478394\n",
            "epoch:  417 loss:  0.2249070703983307\n",
            "epoch:  418 loss:  0.22486776113510132\n",
            "epoch:  419 loss:  0.22482863068580627\n",
            "epoch:  420 loss:  0.22478973865509033\n",
            "epoch:  421 loss:  0.22475109994411469\n",
            "epoch:  422 loss:  0.22471266984939575\n",
            "epoch:  423 loss:  0.2246745228767395\n",
            "epoch:  424 loss:  0.22463653981685638\n",
            "epoch:  425 loss:  0.22459882497787476\n",
            "epoch:  426 loss:  0.22456134855747223\n",
            "epoch:  427 loss:  0.2245241105556488\n",
            "epoch:  428 loss:  0.2244870513677597\n",
            "epoch:  429 loss:  0.2244502454996109\n",
            "epoch:  430 loss:  0.22441363334655762\n",
            "epoch:  431 loss:  0.22437727451324463\n",
            "epoch:  432 loss:  0.22434112429618835\n",
            "epoch:  433 loss:  0.2243051826953888\n",
            "epoch:  434 loss:  0.22426946461200714\n",
            "epoch:  435 loss:  0.2242339551448822\n",
            "epoch:  436 loss:  0.22419863939285278\n",
            "epoch:  437 loss:  0.22416356205940247\n",
            "epoch:  438 loss:  0.22412869334220886\n",
            "epoch:  439 loss:  0.22409406304359436\n",
            "epoch:  440 loss:  0.2240595817565918\n",
            "epoch:  441 loss:  0.22402533888816833\n",
            "epoch:  442 loss:  0.223991259932518\n",
            "epoch:  443 loss:  0.22395740449428558\n",
            "epoch:  444 loss:  0.22392378747463226\n",
            "epoch:  445 loss:  0.22389033436775208\n",
            "epoch:  446 loss:  0.2238571047782898\n",
            "epoch:  447 loss:  0.22382402420043945\n",
            "epoch:  448 loss:  0.2237911820411682\n",
            "epoch:  449 loss:  0.2237584888935089\n",
            "epoch:  450 loss:  0.2237260490655899\n",
            "epoch:  451 loss:  0.22369378805160522\n",
            "epoch:  452 loss:  0.22366167604923248\n",
            "epoch:  453 loss:  0.22362977266311646\n",
            "epoch:  454 loss:  0.22359806299209595\n",
            "epoch:  455 loss:  0.22356654703617096\n",
            "epoch:  456 loss:  0.2235352098941803\n",
            "epoch:  457 loss:  0.22350403666496277\n",
            "epoch:  458 loss:  0.22347307205200195\n",
            "epoch:  459 loss:  0.22344225645065308\n",
            "epoch:  460 loss:  0.2234116643667221\n",
            "epoch:  461 loss:  0.22338123619556427\n",
            "epoch:  462 loss:  0.22335098683834076\n",
            "epoch:  463 loss:  0.2233208864927292\n",
            "epoch:  464 loss:  0.22329097986221313\n",
            "epoch:  465 loss:  0.22326123714447021\n",
            "epoch:  466 loss:  0.223231703042984\n",
            "epoch:  467 loss:  0.22320231795310974\n",
            "epoch:  468 loss:  0.2231730818748474\n",
            "epoch:  469 loss:  0.2231440395116806\n",
            "epoch:  470 loss:  0.22311517596244812\n",
            "epoch:  471 loss:  0.22308647632598877\n",
            "epoch:  472 loss:  0.22305791079998016\n",
            "epoch:  473 loss:  0.22302952408790588\n",
            "epoch:  474 loss:  0.22300131618976593\n",
            "epoch:  475 loss:  0.2229732722043991\n",
            "epoch:  476 loss:  0.22294536232948303\n",
            "epoch:  477 loss:  0.22291763126850128\n",
            "epoch:  478 loss:  0.22289006412029266\n",
            "epoch:  479 loss:  0.2228626310825348\n",
            "epoch:  480 loss:  0.22283540666103363\n",
            "epoch:  481 loss:  0.22280828654766083\n",
            "epoch:  482 loss:  0.22278131544589996\n",
            "epoch:  483 loss:  0.2227545529603958\n",
            "epoch:  484 loss:  0.22272789478302002\n",
            "epoch:  485 loss:  0.22270141541957855\n",
            "epoch:  486 loss:  0.22267507016658783\n",
            "epoch:  487 loss:  0.22264893352985382\n",
            "epoch:  488 loss:  0.2226228564977646\n",
            "epoch:  489 loss:  0.22259698808193207\n",
            "epoch:  490 loss:  0.2225712388753891\n",
            "epoch:  491 loss:  0.22254563868045807\n",
            "epoch:  492 loss:  0.22252020239830017\n",
            "epoch:  493 loss:  0.22249490022659302\n",
            "epoch:  494 loss:  0.2224697321653366\n",
            "epoch:  495 loss:  0.22244469821453094\n",
            "epoch:  496 loss:  0.2224198281764984\n",
            "epoch:  497 loss:  0.222395122051239\n",
            "epoch:  498 loss:  0.22237050533294678\n",
            "epoch:  499 loss:  0.22234602272510529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rYkUBuB2lfiU",
        "outputId": "253d5dd0-6b33-44dd-917e-83aa8a082ad8"
      },
      "source": [
        "# log loss\n",
        "plt.plot(range(epochs), losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80NQ8m48n8fM"
      },
      "source": [
        "# 최종 모델로 test accuracy 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUhXQhwFygw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cbc933-74c0-4b67-92b7-4d42f60b77db"
      },
      "source": [
        "X_test = df_test[['x1','x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']].replace({'?':0})\n",
        "\n",
        "Y_test = df_test['class'].replace({2:0, 4:1})\n",
        "\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "\n",
        "x_test= torch.Tensor(np.array([ X_test['x2'],X_test['x5'] ,X_test['x6'],X_test['x7']]).astype(np.uint8)).t()  # str to unit, [3, 500] ->  [500, 3] X_test['x2'],X_test['x3'], X_test['x7'],\n",
        "#x_test2 = torch.Tensor(np.array([ X_test2['x2'], X_test2['x3'],X_test2['x5'],X_test1['x6'],X_test1['x7']]).astype(np.uint8)).t()\n",
        "print(x_test.shape)\n",
        "# class 2 for benign, 4 for malignant -> 0 for bengn, 1 for malignant\n",
        "y_test = torch.Tensor(Y_test).unsqueeze(1) # [500] -> [500, 1]\n",
        "print(y_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([199, 4])\n",
            "torch.Size([199, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZcMsmzQHERY",
        "outputId": "3b615d67-b35b-45e3-ddc7-6c6e60cbdfb9"
      },
      "source": [
        "#model test w/ all test datasets\n",
        "no_correct = 0\n",
        "for i in range(len(x_train)):\n",
        "  if model.predict(x_train[i]) == y_train[i]:\n",
        "    no_correct += 1\n",
        "\n",
        "accuracy = no_correct/len(x_train)*100\n",
        "print(\"Predcition accuracy_train= {}%\".format(accuracy))\n",
        "\n",
        "no_correct_test=0\n",
        "for i in range(len(x_test)):\n",
        "  if model.predict(x_test[i]) == y_test[i]:\n",
        "    no_correct_test += 1\n",
        "\n",
        "accuracy = no_correct_test/len(x_test)*100\n",
        "\n",
        "print(\"Predcition accuracy_test= {}%\".format(accuracy))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predcition accuracy_train= 93.23308270676691%\n",
            "Predcition accuracy_test= 98.99497487437185%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ounZPqrOyKV"
      },
      "source": [
        "**validation accuracy 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSs3alpzgRlA",
        "outputId": "3656c437-c094-4ddc-8611-ada480898254"
      },
      "source": [
        "no_correct = 0\n",
        "for i in range(len(x_valid)):\n",
        "  if model.predict(x_valid[i]) == y_valid[i]:\n",
        "    no_correct += 1\n",
        "\n",
        "accuracy = no_correct/len(x_valid)*100\n",
        "print(\"Predcition accuracy_valid= {}%\".format(accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predcition accuracy_valid= 98.01980198019803%\n"
          ]
        }
      ]
    }
  ]
}